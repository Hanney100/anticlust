% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/anticlustering-wrapper.R
\name{anticlustering}
\alias{anticlustering}
\title{Balanced anticlustering}
\usage{
anticlustering(features = NULL, distances = NULL, K,
  objective = "distance", method = "sampling", preclustering = TRUE,
  standardize = TRUE, nrep = 10000)
}
\arguments{
\item{features}{A vector, matrix or data.frame of data points. Rows
correspond to elements and columns correspond to features. A
vector represents a single feature.}

\item{distances}{Alternative data argument that can be used if
\code{features} is not used. A N x N matrix representing the
pairwise dissimilarities between all N elements. Larger values
indicate higher dissimilarity. Can be an object of class
\code{dist} (e.g., returned by \code{\link{dist}} or
\code{\link{as.dist}}).}

\item{K}{How many anticlusters should be created.}

\item{objective}{The objective to be maximized. The option "distance"
(default) is used to optimize the anticluster editing objective; 
the option "variance" is used to optimize the k-means objective 
anticlustering objective. See details.}

\item{method}{One of "sampling", or "exact". See details.}

\item{preclustering}{Boolean, should a preclustering be conducted
before anticlusters are created. Defaults to \code{TRUE}. See
details.}

\item{standardize}{Boolean - should the features be standardized
before anticlusters are created? Defaults to \code{TRUE}.
Standardization is done using the function \code{\link{scale}}
using the default settings (mean = 0, SD = 1).}

\item{nrep}{The number of repetitions used for the heuristic method
"sampling". This argument does not have an effect if 
\code{method} is \code{"exact"}.}
}
\value{
A vector representing the anticluster affiliation.
}
\description{
Create equal sized groups of elements (anticlusters) that are as
similar as possible.
}
\details{
This function is used to solve balanced K anticlustering. That is, K
groups of equal size are created in such a way that similarity of
these groups is maximized. Similarity is measured by one of two
objective functions. Späth (1986) and Valev (1998) proposed to
maximize the variance criterion used in k-means clustering to
establish similar groups in the anticlustering application.
Optimizing the variance criterion (thus, solving k-means
anticlustering) is accomplished by setting \code{objective =
"variance"}. 
The \code{anticlust} package also introduces another objective
function to the anticlustering application that has been developed in
the problem domain of cluster editing. It is based on a measure of
the pairwise distances of data points (Grötschel & Wakabayashi,
1989). Anticluster editing maximizes the sum of pairwise distances
within anticlusters. The anticluster editing objective can be
optimized through an exact integer linear program. To obtain the
optimal solution, a linear programming solver must be available on
the system and usable from R. Support is available for the open
source GNU linear programming kit (called from the package
\code{Rglpk}) and the commercial solvers gurobi (called from the
package \code{gurobi}) and IBM CPLEX (called from the package
\code{Rcplex}). A license is needed to use one of the commercial
solvers. The optimal solution is retrieved by setting \code{objective
= "distance"}, \code{preclustering = FALSE}, and \code{method =
"exact"}. Use this combination of arguments only for small problem
sizes (maybe <= 30 elements). No algorithm is available to find the
optimal solution for k-means anticlustering.
To relax the optimality condition, it is possible to set
\code{preclustering = TRUE}. In this case, the distance objective is
still optimized using integer linear programming, but a preprocessing
forbids very similar elements to be assigned to the same anticluster.
This approach can be used to work on larger problem instances and the
solution is usually still optimal or very close to optimal.

If no exact solution is required or the problem size is too large for
the exact approach, a heuristic method based on repeated random
sampling is available. Across a specified number of runs,
anticlusters are assigned randomly and the best assignment
(maximizing the sum of within-group distances) is returned. This
method works for both anticluster editing and k-means anticlustering
(the latter is accomplished by setting `objective = "variance"`). The
sampling approach may also incorporate a preclustering that prevents
grouping very similar elements into the same anticluster; use
\code{preclustering = TRUE} to activate this option, which is also
the default. It is suggested that the preclustering condition is
activated for the random sampling approach because it usually
improves the quality of the solution.
}
\examples{

## Use anticlustering on the iris data set with the default settings:
# (a) Optimizes the distance objective
# (b) Heuristic method: random sampling
# (c) 10,000 sampling repetitions
# (d) Preclustering is activated
# (e) Anticlustering uses standardized features (each feature has 
#     mean = 0 and SD = 1)

data(iris)
# Only use numeric attributes
anticlusters <- anticlustering(iris[, -5], K = 3)
# Compare feature means by anticluster
by(iris[, -5], anticlusters, function(x) round(colMeans(x), 2))
# Plot the anticlustering
par(mfrow = c(1, 2))
plot_clusters(iris[, 1:2], anticlusters)
plot_clusters(iris[, 3:4], anticlusters)

## Exact anticlustering
# Create artifical data
n_features <- 2
n_elements <- 20
K <- 2
features <- matrix(rnorm(n_elements * n_features), ncol = n_features)
anticlustering(features, K = K, method = "exact",
               preclustering = FALSE, standardize = FALSE)

# Enable preclustering
anticlustering(features, K = K, method = "exact",
               preclustering = TRUE, standardize = FALSE)

}
\references{
M. Grötschel and Y. Wakabayashi, “A cutting plane algorithm for a
clustering problem,” Mathematical Programming, vol. 45, nos. 1-3, pp.
59–96, 1989.

H. Späth, “Anticlustering: Maximizing the variance criterion,”
Control and Cybernetics, vol. 15, no. 2, pp. 213-218, 1986.

Valev, V. (1998). Set partition principles revisited. In Joint IAPR
international workshops on statistical techniques in pattern
recognition (SPR) and structural and syntactic pattern recognition
(SSPR) (pp.  875–881).
}
