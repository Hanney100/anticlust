---
title: "Getting started with the anticlust package"
author: "Martin Papenberg"
date: "`r Sys.Date()`"
output:
  md_document:
    variant: markdown_github

bibliography: lit.bib
---

# anticlust

`anticlust` is an `R` package that is used for »anticlustering«.
Anticlustering is a method to assign elements to sets in such a way that
the sets are as similar as possible [@spath1986; @valev1998]. The
package `anticlust` was originally developed to assign items to
experimental conditions in experimental psychology, but it can be
applied whenever similar sets are desired. Currently, `anticlust`
supports the possibility to create sets of equal size. 

```{r setup, include = FALSE}
library("knitr")
library("anticlust")
library("dplyr")

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.cap = "",
  message = FALSE,
  warning = FALSE
)

## Function to print descriptive statistics by anticluster
print_table <- function(features, anticlusters) {
  features$Anticluster <- anticlusters
  M <- features %>%
    group_by(Anticluster) %>%
    summarize_all(list(mean)) %>%
    round(2)
  SD <- features %>%
    group_by(Anticluster) %>%
    summarize_all(list(sd)) %>%
    round(2)
  tab <- rbind(M, SD)
  tab_ <- apply(tab[, -1], 2, prmisc::force_decimals)
  n_clusters <- length(unique(anticlusters))
  empty <- rep("", n_clusters - 1)
  funrow <- c("Mean", empty, "SD", empty)
  kable(data.frame(Statistic = funrow, tab_, Anticluster = tab$Anticluster))
}

set.seed(12345)
```

## Installation

```R
library("devtools") # if not available: install.packages("devtools")
install_github("m-Py/anticlust")
```

```{r}
# load the package via
library("anticlust")
```

## How do I learn about anticlustering

This page contains a quick start on how to employ anticlustering using
the `anticlust` package. So, you should start by simply continuing to
read. More information is available via the following sources: 

1. The R help. The main function of the package is `anticlustering()`
and the help page of the function (`?anticlustering`) is useful to learn
more about anticlustering. It provides explanations of all function
parameters and how they relate to the theoretical background of
anticlustering.

2. I created a repository on the [Open Science
Framework](https://osf.io/cd5sr/) that includes materials for a better
understanding of the anticlustering method. Currently, it contains the
slides of a talk that I gave a the TeaP conference (Annual meeting of
Experimental Psychologists) in London in April, 2019. The slides can be
retrieved [here](https://osf.io/jbthk/); they contain a visual
illustration of the anticlustering method and example code for different
applications.

3. There is a paper in preparation that will explain the theoretical
background of the `anticlust` package in detail.

4. If you have any question on the anticlustering method and the
`anticlust` package, I encourage you to contact me via email
(martin.papenberg@hhu.de) or [Twitter](https://twitter.com/MPapenberg)
or to open an issue on this Github repository.

## A quick start

We can use the function `anticlustering()` to create similar sets of
elements. It takes as input a data table describing the elements that
should be assigned to sets. In the data table, each row represents an
element, for example a person, word or a photo. Each column is a numeric
variable describing one of the elements' features. The table may be an R
`matrix` or `data.frame`; a single feature can also be passed as a
`vector`.

To illustrate the usage of the `anticlustering` function, we use the
classical iris data set describing the characteristics of 150 iris
plants:

```{r}
## Select only the numeric attributes
features <- iris[, -5]
nrow(features)
```

The first rows of the data set look as follows:

```{r, echo = FALSE}
kable(head(features))
```

We now use the `anticlustering` function to create two similar groups
of iris plants:

```{r}
anticlusters <- anticlustering(features, K = 2, standardize = TRUE)
anticlusters
table(anticlusters)
```

To get an intuition for how an anticluster looks like, we first plot the
plants' characteristics by anticluster:

```{r, fig.width = 6.5}
par(mfrow = c(1, 2))
pch <- 15:16
# plot_clusters is also a function of the anticlust package
plot_clusters(features[, 1:2], anticlusters, pch = pch) 
plot_clusters(features[, 3:4], anticlusters, pch = pch)

```

This looks rather chaotic, but it is probably what we want: We want a
strong overlap in all of the plants' characteristics because the
anticlusters should be similar. In addition to visually inspecting the
anticlustering plots, we want to investigate the descriptive statistics
of the plants' characteristics like the mean and standard deviation by
anticluster. Ideally, the values should be the same for each
anticluster. As we can see in the following, this worked quite well: 

```{r, echo = FALSE}
print_table(features, anticlusters)
```

## The anticlustering objective

In the example above, the `anticlustering` function established
anticlusters that were very similar with regard to the mean and standard
deviation of each plant feature. However, it was just a side effect that
group means turned out to be similar -- the anticlustering method does
not directly minimize differences in groups means. Instead,
anticlustering makes use of two measures of set similarity that have
been developed in the context of cluster analysis: 

- the k-means "variance" objective [@spath1986; @valev1998] 
- the cluster editing "distance" objective [@bocker2013; @miyauchi2015; @grotschel1989]

The k-means objective is given by the sum of the squared errors between
cluster centers and individual data points [@jain2010]. The cluster
editing objective is the sum of pairwise distances within anticlusters.
Maximizing either of these objectives leads to similar groups, i.e.,
anticlusters. Minimization of the same objectives leads to a clustering,
i.e., elements are as similar as possible within a set and as different
as possible between sets. Thus, anticlustering is generally accomplished 
by maximizing the spread of the data in each group, whereas clustering 
minimizes the spread.

To vary the objective function, we may change the parameter `objective`.
To apply anticluster editing, use `objective = "distance"`, which is
also the default. To maximize the k-means variance objective, set
`objective = "variance"`. In many cases, the results for the
`"variance"` objective (k-means) and the `"distance"` objective
(anticluster editing) will be quite similar. The following shows the
results of the maximizing the variance objective on the iris data:

```{r}
anticlusters <- anticlustering(features, K = 2, standardize = TRUE,
                               objective = "variance")
```

```{r, echo = FALSE}
print_table(features, anticlusters)
```

## Exact anticluster editing

Finding an optimal assignment of elements to sets that maximizes the
anticluster editing or variance objective is computationally demanding.
For anticluster editing, the package `anticlust` still offers the
possibility to find the best possible assignment, relying on [integer
linear programming](https://en.wikipedia.org/wiki/Integer_programming).
This exact approach employs a formulation developed by @grotschel1989,
which has been used to rather efficiently solve the cluster editing
problem [@bocker2011]. To obtain an optimal solution, a linear
programming solver must be installed on your system; `anticlust`
supports the commercial solvers [gurobi](https://www.gurobi.com/) and
[CPLEX](https://www.ibm.com/analytics/cplex-optimizer) as well as the
open source [GNU linear programming
kit](https://www.gnu.org/software/glpk/glpk.html). The commercial
solvers are generally faster. Researchers can install a commercial
solver for free using an academic licence. To use any of the solvers
from within `R`, one of the interface packages `gurobi` (is shipped with
the software gurobi),
[Rcplex](https://CRAN.R-project.org/package=Rcplex) or
[Rglpk](https://CRAN.R-project.org/package=Rglpk) must also be
installed.

To find the optimal solution, we have to set the arguments `method =
"ilp"`:

```R
anticlustering(features, K = 2, method = "ilp")
```

## Preclustering

The exact integer linear programming approach will only work for small
problem sizes (< 30 elements). We can increase the problem size that can
be handled by setting the argument `preclustering = TRUE`. In this case,
an initial cluster analysis performed, creating small groups of elements
that are very similar. The preclustering step identifies pairs of
similar stimuli if K = 2, triplets if K = 3, and so forth. After this
preclustering, a restriction is enforced to the integer linear program
that precludes very similar elements to be assigned to the same set.
This procedure is illustrated in the following plot for K = 2 and n =
10:

```{r, out.width = '60%', echo = FALSE}
knitr::include_graphics("./README_files/preclustering.jpg")
```

The preclustering restrictions improve the running time of the integer
linear programming solver by a large margin (often 100x as fast) because
many possible assignment are rendered illegal; the integer linear
programming solver is smart and disregards these assignments from the
space of feasible assignments. In some occasions, these restrictions
prohibit the integer linear programming solver to find the very best
partitioning (i.e., the assignment with the maximum distance /
variance), because this may be only obtained when some of the
preclustered elements are assigned to the same group. However, in
general, the solution is still very good and often optimal. This code
can be used to employ integer linear programming under preclustering
constraints.

```R
anticlustering(features, K = 2, method = "ilp", preclustering = TRUE)
```

## Random search

To solve larger problem instances that cannot be processed using integer
linear programming, a heuristic method based on random sampling is
available. Across a user-specified number of runs (specified via the
argument `nrep`), each element is first randomly assigned to an
anticluster and then the objective value is computed. In the end, the
best assignment is returned as output. To activate the heuristic, set
`method = "heuristic"` (this is also the default argument). When we set
`preclustering = TRUE`, the random assignment is conducted under the
restriction that preclustered elements cannot be part of the same
anticluster. In my experience, the preclustering restrictions often
improve the output of the random sampling approach, because the
preclustering itself serves as a useful heuristic: when very similar
items are guaranteed to be in different sets, these different sets tend
to become similar. 

## References
