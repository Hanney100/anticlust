---
title: "Getting started with the anticlust package"
author: "Martin Papenberg"
date: "`r Sys.Date()`"
output:
  md_document:
    variant: gfm
  #html_document
bibliography: lit.bib
---

# anticlust

`anticlust` is an `R` package for »anticlustering«, a method to assign
elements to sets in such a way that the sets are as similar as possible.
The package `anticlust` was originally
developed to assign items to experimental conditions in experimental
psychology, but it can be applied whenever a user requires that a given
set of elements has to be partitioned into similar subsets. The 
`anticlust` package offers the possibility to create sets that are of
equal size (which is the standard case), but it is also possible to create sets 
of different size, or to only assign a subset of all elements to a set.
The package is still under active developement; expect
changes and improvements before it will be submitted to CRAN. Check out
the [NEWS file](https://m-py.github.io/anticlust/NEWS.html) 
for recent changes.

```{r setup, include = FALSE}
library("anticlust")

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.cap = "",
  message = FALSE,
  warning = FALSE
)

set.seed(123)

```

## Installation

```R
library("remotes") # if not available: install.packages("remotes")
install_github("m-Py/anticlust")
```

## Example

In this initial example, I use the main function `anticlustering()` to
create three similar sets of plants using the classical iris data set:

```{r}
# load the package via
library("anticlust")

anticlusters <- anticlustering(
  iris[, -5],
  K = 3,
  objective = "variance",
  method = "exchange"
)

## The output is a vector that assigns a group (i.e, a number 
## between 1 and K) to each input element:
anticlusters

## Each group has the same number of items:
table(anticlusters)

## Compare the feature means by anticluster:
by(iris[, -5], anticlusters, function(x) round(colMeans(x), 2))
```

## How do I learn about anticlustering

This page contains some basic information on anticlustering. 
So, you might start by simply continuing to read. More information 
is available via the following sources:

1. There is a preprint available (»Using anticlustering to partition a 
   stimulus pool into equivalent parts«) describing the theoretical 
   background of anticlustering and the `anticlust` package in detail. 
   It can be retrieved from [https://psyarxiv.com/3razc/](https://psyarxiv.com/3razc/)

2. Use the R help. The main function of the package is `anticlustering()`
  and the help page of the function (`?anticlustering`) is useful to learn
  more about anticlustering. It provides explanations of all function
  parameters and how they relate to the theoretical background of
  anticlustering.

3. I intend to write some vignettes on typical usages of the 
  `anticlust` package in the near future. Stay tuned.

## A quick start

As illustrated in the example above, we can use the function
`anticlustering()` to create similar sets of elements. The function
takes as input a data table describing the elements that should be
assigned to sets. In the data table, each row represents an element, for
example a person, word or a photo. Each column is a numeric variable
describing one of the elements' features. The table may be an R `matrix`
or `data.frame`; a single feature can also be passed as a `vector`. The
number of groups is specified through the argument `K`.

To quantify set similarity, `anticlust` may employ one of two measures
that have been developed in the context of cluster analysis:

- the k-means "variance" objective
- the cluster editing "distance" objective

The k-means objective is given by the sum of the squared distances
between cluster centers and individual data points. The
cluster editing objective is the sum of pairwise distances within each
anticluster.  The following plot illustrates both objectives for 15 
elements that have been assigned to three sets. Each element is 
described by two numeric features, displayed as the *x* and *y* axis:

```{r, out.width = '100%', echo = FALSE}
knitr::include_graphics("objectives_updated.png")
```


```{r, echo = FALSE, eval = FALSE}

## Create N random elements:
N <- 12
features <- matrix(rnorm(N * 2), ncol = 2)
K <- 3

## Generate all possible partitions to divide N items in K sets:
partitions <- generate_partitions(K, N)

## Create an objective function that takes the partition
## as first argument (then, we can use sapply to compute
## the objective for each partition)
var_obj <- function(clusters, features) {
  variance_objective(features, clusters)
}

dist_obj <- function(clusters, features) {
  distance_objective(features, clusters = clusters)
}

var_objectives <- sapply(
  partitions,
  FUN = var_obj,
  features = features
)

dist_objectives <- sapply(
  partitions,
  FUN = dist_obj,
  features = features
)

## Select the best partitions:
max_var <- partitions[var_objectives == max(var_objectives)][[1]]
min_var <- partitions[var_objectives == min(var_objectives)][[1]]
max_dist <- partitions[dist_objectives == max(dist_objectives)][[1]]
min_dist <- partitions[dist_objectives == min(dist_objectives)][[1]]

cols <- c("#ff0000", "#1f5a07", "#ABCDEF")

## Plot minimum and maximum objectives:
par(mfrow = c(2, 2))
plot_clusters(
  features,
  max_var,
  illustrate_variance = TRUE,
  main = "Maximum variance",
  col = cols,
  pch = 3:5
)
plot_clusters(
  features,
  max_dist,
  within_connection = TRUE,
  main = "Maximum distance",
  col = cols,
  pch = 3:5
)
plot_clusters(
  features,
  min_var,
  illustrate_variance = TRUE,
  main = "Minimum variance",
  col = cols,
  pch = 3:5
)
plot_clusters(
  features,
  min_dist,
  within_connection = TRUE,
  main = "Minimum distance",
  col = cols,
  pch = 3:5
)

```

The lines connecting the dots illustrate the distances that enter the
objective functions. For anticluster editing ("distance objective"),
lines are drawn between pairs of elements within the same anticluster,
because the objective is the sum of the pairwise distances between
elements in the same cluster.  For k-means anticlustering ("variance
objective"), lines are drawn between each element and the cluster
centroid, because the objective is the sum of the squared distances
between cluster centers and elements.

Minimizing either the distance or the variance objective creates three
distinct clusters of elements (as shown in the upper plots), whereas
maximization leads to a strong overlap of the three sets, i.e., three
anticlusters (as shown in the lower plots). For anticlustering, the
distance objective maximizes the average similarity between elements in
different sets, whereas the variance objective tends to maximize the
similarity of the cluster centers (i.e., the feature means).

To vary the objective function in the `anticlust` package, we can change
the parameter `objective`. To use anticluster editing, use
`objective = "distance"` (this is also the default). To maximize the
k-means variance objective, set `objective = "variance"`.

```{r, eval = FALSE}

## Example code for varying the objective:
anticlustering(
  features, 
  K = 3, 
  objective = "distance"
)

anticlustering(
  features, 
  K = 3, 
  objective = "variance"
)
```

### Categorical constraints

Sometimes, it is required that sets are not only similar with regard to
some numeric variables, but we also want to ensure that each set
contains an equal number of elements of a certain category. Coming back
to the initial iris data set, we may want to require that each set has a
balanced number of plants of the three iris species. To this end, we can
use the argument `categories` as follows:

```{r}
anticlusters <- anticlustering(
  iris[, -5],
  K = 3,
  objective = "variance",
  method = "exchange",
  categories = iris[, 5]
)

## The species are as balanced as possible across anticlusters:
table(anticlusters, iris[, 5])

```

## Questions and suggestions

If you have any question on the `anticlust` package or any suggestions
(which are greatly appreciated), I encourage you to contact me via email
(martin.papenberg@hhu.de) or [Twitter](https://twitter.com/MPapenberg),
or to open an issue on this Github repository.

## Reference

Papenberg, M., & Klau, G. W. (2019, October 30). Using anticlustering to
partition a stimulus pool into equivalent parts. 
[https://doi.org/10.31234/osf.io/3razc](https://doi.org/10.31234/osf.io/3razc)
