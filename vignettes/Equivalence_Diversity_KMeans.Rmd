---
title: "On the equivalence of the k-means objective and the diversity objective"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{On the equivalence of the k-means objective and the diversity objective}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo = FALSE}
library(anticlust)
```

The input of k-means anticlustering is an $N \times M$ data table with $N$ cases and $M$ variables. If we denote this structure as $\mathbf{X}_{N \times M} = \{x_{im}\}_{N \times M}$, k-means anticlustering maximizes the error sum of squares ($\mathit{SSE}$): 

$$
\mathit{SSE} = 
  \sum\limits_{m=1}^{M} 
  \sum\limits_{k=1}^{K} 
  \sum\limits_{i \in C_k}
  (x_{im} - \overline{x}_m ^{(k)})^2
$$

The set $C_k$ contains the indices of all elements belonging to the *k*'th cluster ($k = 1, \dots K$). The vector $\overline{\mathbf{x}}^{(k)} = (\overline{x}_1^{(k)}, \overline{x}_2^{(k)} \ldots, \overline{x}_M^{(k)})$ is the centroid of the $k$'th cluster where $\overline{x}_j ^{(k)}$ is defined as

$$
\overline{x}_j ^{(k)} = \frac{1}{n_k} \sum\limits_{i \in C_k} x_{ij}.
$$

The number of elements in the $k$'th group is denoted by $n_k$. Thus, the $\mathit{SSE}$ is computed as the sum of the squared Euclidean distances between all data points and the centroid of their respective cluster. 

According to Brusco (2006), the k-means criterion can also be expressed using the pairwise squared Euclidean distances between data points. We define a distance matrix $\mathbf{D}_{N \times N} = \{d_{ij}\}_{N \times N}$ where $d_{ij}$ is the squared Euclidean distance between the $i$'th and $j$'th object:

$$
  d_{ij} = \sum\limits_{m = 1}^{M}(x_{im} - x_{jm})^2
$$

Based on $\mathbf{D}_{N \times N}$, the k-means criterion is then computed as 

$$
\mathit{SSE} = \sum\limits_{k = 1}^{K}\left( \frac{1}{n_k} \sum_{(i<j)\in C_k} d_{ij} \right)
$$

That is, "the sum of squared distances of a collection of points from the centroid associated with those points is equal to the sum of the pairwise squared distances between those points divided by the size (number of objects) of the collection" (Brusco, 2006, p. 350).

An important special case of anticlustering applications is that all groups are equal-sized, i.e., $n_k = \frac{N}{K}$ ($k = 1, \dots K$). Here, we obtain 

$$
\mathit{SSE} = \frac{N}{K} \sum\limits_{k = 1}^{K} \sum_{(i<j)\in C_k} d_{ij}
$$

In this case, the normalizing factor $\frac{N}{K}$ can be ignored when maximizing $\mathit{SSE}$. Thus, the criterion reduces to the sum of the (squared) distances between elements in the same cluster: 

$$
\mathit{SSE_{Diversity}} = \sum\limits_{k = 1}^{K}\sum_{(i<j)\in C_k} d_{ij}
$$
This term corresponds to the standard diversity objective, with the distances $d_{ij}$ representing squared Euclidean distances.

Because maximizing $\mathit{SSE_{Diversity}}$ maximizes $\mathit{SSE}$, algorithms maximizing the diversity can also be used to maximize the k-means criterion (at least when requiring equal sized groups). The following code shows how it can be done using `anticlust`:

```{r}
features <- schaper2019[, 3:6]
dists_squared <- dist(features)^2

K <- 6
initial_clusters <- sample(rep_len(1:K, nrow(features)))

cl1 <- anticlustering(features, K = initial_clusters, objective = "variance")
cl2 <- anticlustering(dists_squared, K = initial_clusters, objective = "diversity")

all(cl1 == cl2)
```


## References

Brusco, M. J. (2006). A repetitive branch-and-bound procedure for minimum within-cluster sums of squares partitioning. *Psychometrika, 71*, 347--363.

