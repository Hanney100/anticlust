---
title: "Getting started with the anticlust package"
author: "Martin Papenberg"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

bibliography: lit.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.cap = "",
  message = FALSE,
  warning = FALSE
)

set.seed(123)
```

Clustering algorithms establish groups of elements ensuring that
elements within each cluster are homogenoues, but clearly separated from
elements in  other clusters. Anticlustering reverses this logic and
creates groups of elements (anticlusters) that are as similar as
possible [@spath1986; @valev1998]. The `R` package `anticlust` tackles
this problem algorithmically.

## A quick start

As a start we see the main function `anticlustering` in action (for most
users of the package `anticlust`, it should be sufficient to know this
function). As is the common case for clustering functions (for example
see `?kmeans`), the `anticlustering` function takes a data matrix of
features describing the elements that we wish to assign to different
groups. Each row is any kind of element element, for example a person, a
picture, a word or a photo. Each column is a variable describing one of
the  elements' features. To illustrate the usage of the package, we use
the classical iris data set describing the properties of 150 iris plants:

```{r}
library("knitr")

## Select data for
data <- iris[, -5]
kable(head(data))
nrow(data)
```

We now use the `anticlustering` function to establish 3 similar groups
of iris plants:

```{r}
library("anticlust")
n_clusters <- 3
anticlusters <- anticlustering(data, n_clusters)
anticlusters
table(anticlusters)
```

The vector `anticlusters` contains the anticluster affiliation of each
plant. The `anticlustering` function created an equal number of plants
per anticluster. Currently, it is not possible to create anticlusters
of different sizes, but it may well be in the future.

We wish to know how well the anticluster assignment worked. First, we
plot plant characteristics by anticluster:

```{r, fig.width = 6.5}
par(mfrow = c(1, 2))
plot_clusters(data[, 1:2], anticlusters)
plot_clusters(data[, 3:4], anticlusters)
```

This looks rather chaotic, but it is probably what we want: Among the
anticlusters, we expect a strong overlap in all of the plants'
characteristics. In constrast, a clustering of plants have created a
strong separation between groups, as can be seen in this example:

```{r, fig.width = 6.5}
clusters <- clustering(data, n_clusters)

par(mfrow = c(1, 2))
plot_clusters(data[, 1:2], clusters)
plot_clusters(data[, 3:4], clusters)
```

As the name suggests, anticlustering is the reversal of clustering.
Instead of separating groups as effectvely as possible, we create a
strong overlap between groups; groups should at best be
indistinguishable.

In addition to visually inspecting the plots above, we probably want to
know how well the anticlustering worked by investigating the descriptive
statistics of the plants' characteristics by anticluster. Ideally, the
distribution of plant characteristics should be the same for each
anticluster. This code example inspects the mean values of each feature
by anticluster.

```{r}
library("dplyr")

data <- data.frame(Anticluster = anticlusters, data)
data %>%
  group_by(Anticluster) %>%
  summarize_all(funs(mean)) %>%
  round(2) %>%  knitr::kable()
```

A completely random assignment very likely produces a much worse
grouping:

```{r}
data$Anticluster <- rep(1:n_clusters, nrow(data) / n_clusters)
data$Anticluster <- sample(data$Anticluster)

data %>%
  group_by(Anticluster) %>%
  summarize_all(funs(mean)) %>%
  round(2) %>% knitr::kable()
```

## The objective function

In the example above, we noted that the `anticlustering` function
established anticlusters that were very similar with regard to the mean
of each plant feature. In particular, the approach vastly outperformed a
completely random assignment of anticlusters.

However, the anticlustering method does not actually try to minimize
differences in groups means -- it is just a side effect that group means
turn out to be similar. Instead, anticlustering uses optimization
objectives that have been developed for clustering methods, and reverses
their logic. In particular, the package `anticlust`, adapts two
clustering methods: k-means [@spath1986; @valev1998] and cluster editing
[@bocker2013]. K-means is the most classical of all clustering
algorithms [@jain2010]. Across a pre-specified number of clusters (*k*),
the k-means algorithm tries to minimize the variance within all
clusters. @spath1986 as well as @valev1998 proposed to maximize the
variance criterion to establish groups that are similar to each other.
Cluster editing is based on the pairwise mathematical distances between
all elements, for example the classical euclidean distance. The cluster
editing objective is to find a grouping that minimizes the sum of
dissimilarities across all groups [@grotschel1989]. When using the
cluster editing objective in an anticlustering application, we try to
maximize the sum of all pairwise distances within all groups. These
objectives are discussed in more detail in the vignette "Technical
notes" that is included with the package.

To vary the objective function, we can use the parameter `objective` of
the function `anticlustering`. In most cases, the results for the
variance criterion (รก la k-means) and the results for the distance
criterion (รก la cluster editing) will be quite similar. The default
objective is `"distance"`. We can change the objective to `"variance"`
as follows:

```{r, fig.width = 6.5}
data <- iris[, -5]
data$Anticluster <- anticlustering(data, n_clusters, objective = "variance")

data %>%
  group_by(Anticluster) %>%
  summarize_all(funs(mean)) %>%
  round(2) %>% knitr::kable()
```

## How to procede

If you are already happy with the results provided by the
`anticlustering` function, you may stop here. If you wish to learn more
about the theory of anticlustering, you may procede to the vignette
"Technical notes" that is included with the package. It provides a more
detailed explanation of the variance and distance objectives and the
algorithms the package uses to optimize the objectives. The help
page (`?anticlustering`) explains all of the parameters that can be
adjusted for the `anticlusting` function.

## References
