---
title: "Algorithms for the anticlustering problem"
author: "Martin Papenberg"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.cap = ""
)
```

Clustering algorithms establish groups of elements ensuring that
elements within each cluster are homogenoues, but clearly separated from
element in  different clusters. Anticlustering reverses this logic:
anticlustering establishes groups of elements (anticlusters) that are as
similar as possible (Spaeth, 1987). This is useful in several contexts,
for example to

- assign students to groups of equal prior achievement
- create examinations of equal difficulty for different cohorts of students
- ...

Building on the theory on cluster analysis, the `R` package `anticlust`
package tackles such applications algorithmically.

## A quick start

First, we get to see the main function `anticlustering` in action. As
for many clustering algorithms (for example see `?kmeans` from
base-`R`), the `anticlustering` function takes a data matrix of features
describing our elements. Each row is an element, for example a student,
and each column is a variable. We use the classical iris data set:

```{r}
library("knitr")

## Select data for
data <- iris[, -5]
kable(head(data))
nrow(data)
```

We now use the `anticlustering` function to establish 3 similar groups
of iris plants:

```{r}
library("anticlust")
n_clusters <- 3
anticlusters <- anticlustering(data, n_clusters, standardize = TRUE)
anticlusters
table(anticlusters)
```

The vector `anticlusters` now contains the anticluster affiliation, of
each plant. The `anticlustering` function created an equal number of
plants per anticluster.

Now, we wish to know how well the assignment worked. First, we plot
plant characteristics by anticluster:

```{r}
draw_assignment(data[, 1:2], anticlusters, cex = 1)
draw_assignment(data[, 3:4], anticlusters, cex = 1)
```

This looks rather chaotic, but it is probably what we want: We expect a
strong overlap in all characteristics of the plants. In
constrast, a clustering of plants have looked like this:

```{r}
clusters <- kmeans(data, n_clusters)$cl
clusters
draw_assignment(data[, 1:2], clusters, cex = 1)
draw_assignment(data[, 3:4], clusters, cex = 1)
```

As the name suggests, anticlustering is the reversal of clustering.
Instead of separating groups as well as possible, we create a strong
overlap of anticlusters; groups should at best be indistinguishable.

In addition to just inspecting the plot above, we probably want to know
how well the anticlustering worked by investigating the descriptive
statistics of features by anticluster. Ideally, the distribution of
plant characteristics should be the same for each anticluster.

```{r}
library("dplyr")

data <- data.frame(class = anticlusters, data)
data %>%
  group_by(class) %>%
  summarize_all(funs(mean, sd)) %>%
  round(2) %>%
  knitr::kable()

```

A completely random assignment would very likely produce a much worse
grouping:

```{r}

data$class <- rep(1:n_clusters, nrow(data) / n_clusters)
data$class <- sample(data$class)

data %>%
  group_by(class) %>%
  summarize_all(funs(mean, sd)) %>%
  round(2) %>%
  knitr::kable()

```

## How does it work

- Explain the anticlustering principle - vs. k-means and cluster editing
- Objective functions

## Algorithms

Computational complexity

### Exact

Integer linear programming

### Heuristics

Simulated annealing
